# Reward è®¾è®¡é—®é¢˜åˆ†æ

## ğŸ”´ å‘ç°çš„ä¸¥é‡é—®é¢˜

### é—®é¢˜1ï¼šè‡ªé€‚åº” sigma è®¾è®¡åç›´è§‰ï¼ˆæœ€ä¸¥é‡ï¼‰

**ä½ç½®**ï¼š`_reward_tracking_ee_orientation_6d_base` (2259-2261è¡Œ) å’Œ `_reward_tracking_ee_force_base` (2309-2310è¡Œ)

**å½“å‰å®ç°**ï¼š
```python
sigma_scale = 0.3 + 0.7 * (diff_per_finger / (diff_per_finger + adaptive_threshold))
adaptive_sigma = torch.clamp(base_sigma * sigma_scale, min=base_sigma * 0.3, max=base_sigma * 1.0)
rew_per_finger = torch.exp(-diff_per_finger / (adaptive_sigma + 1e-6) * 2)
```

**é—®é¢˜åˆ†æ**ï¼š
- **è¯¯å·®å¤§æ—¶ï¼Œsigma å˜å¤§**ï¼šè¿™ä¼šå¯¼è‡´å¥–åŠ±è¡°å‡å˜æ…¢
- **åç›´è§‰**ï¼šè¯¯å·®å¤§åº”è¯¥å¾—åˆ°æ›´å°‘çš„å¥–åŠ±ï¼Œä½†å½“å‰è®¾è®¡è®©è¯¯å·®å¤§æ—¶å¥–åŠ±è¡°å‡æ›´æ…¢
- **Value function éš¾ä»¥å­¦ä¹ **ï¼šreward scale åœ¨ä¸åŒçŠ¶æ€ä¸‹å˜åŒ–å¾ˆå¤§ï¼Œå¯¼è‡´ value function é¢„æµ‹å›°éš¾

**æ•°å­¦åˆ†æ**ï¼š
- å½“ `diff_per_finger = 0` æ—¶ï¼š`sigma_scale = 0.3`ï¼Œ`adaptive_sigma = 0.3 * base_sigma`ï¼ˆæœ€å°ï¼‰
- å½“ `diff_per_finger â†’ âˆ` æ—¶ï¼š`sigma_scale â†’ 1.0`ï¼Œ`adaptive_sigma = base_sigma`ï¼ˆæœ€å¤§ï¼‰
- è¿™æ„å‘³ç€ï¼š**è¯¯å·®è¶Šå¤§ï¼Œsigma è¶Šå¤§ï¼Œå¥–åŠ±è¡°å‡è¶Šæ…¢** âŒ

**å½±å“**ï¼š
- Value function éœ€è¦å­¦ä¹ ä¸€ä¸ªéå¸¸å¤æ‚çš„ reward åˆ†å¸ƒ
- ä¸åŒè¯¯å·®çŠ¶æ€ä¸‹ reward scale ä¸åŒï¼Œå¯¼è‡´ value loss æŒ¯è¡

### é—®é¢˜2ï¼šReward å…¬å¼ä¸­çš„ç³»æ•° 2 å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®š

**ä½ç½®**ï¼š2264è¡Œå’Œ2313è¡Œ

```python
rew_per_finger = torch.exp(-diff_per_finger / (adaptive_sigma + 1e-6) * 2)
```

**é—®é¢˜**ï¼š
- ç³»æ•° `* 2` ä¼šè®©å¥–åŠ±è¡°å‡æ›´å¿«
- ç»“åˆè‡ªé€‚åº” sigmaï¼Œå¯èƒ½å¯¼è‡´æŸäº›çŠ¶æ€ä¸‹ reward æ¥è¿‘ 0ï¼Œæ¢¯åº¦æ¶ˆå¤±
- ä¸åŒçŠ¶æ€ä¸‹ reward çš„ scale å·®å¼‚å¾ˆå¤§

### é—®é¢˜3ï¼šå¤šæ‰‹æŒ‡å¹³å‡å¯èƒ½æ©ç›–é—®é¢˜

**ä½ç½®**ï¼š2267è¡Œå’Œ2316è¡Œ

```python
rew = torch.mean(rew_per_finger, dim=1)  # shape: (num_envs,)
```

**é—®é¢˜**ï¼š
- å¦‚æœæŸäº›æ‰‹æŒ‡è·Ÿè¸ªå¾ˆå¥½ï¼ˆreward â‰ˆ 1ï¼‰ï¼ŒæŸäº›å¾ˆå·®ï¼ˆreward â‰ˆ 0ï¼‰ï¼Œå¹³å‡åå¯èƒ½ â‰ˆ 0.5
- è¿™ä¼šè®© agent è®¤ä¸ºå½“å‰çŠ¶æ€è¿˜å¯ä»¥ï¼Œä½†å®é™…ä¸ŠæŸäº›æ‰‹æŒ‡éœ€è¦æ”¹è¿›
- å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š

### é—®é¢˜4ï¼šReward scale é…ç½®å¯èƒ½ä¸åˆé€‚

**é…ç½®**ï¼š
```python
tracking_ee_force_base = 1.0
tracking_ee_orientation_6d_base = 0.5
```

**é—®é¢˜**ï¼š
- å¦‚æœ reward æœ¬èº«çš„å€¼åŸŸæ˜¯ [0, 1]ï¼Œscale 1.0 å’Œ 0.5 æ˜¯åˆç†çš„
- ä½†ç”±äºè‡ªé€‚åº” sigmaï¼Œå®é™… reward çš„å€¼åŸŸå¯èƒ½å˜åŒ–å¾ˆå¤§
- éœ€è¦æ£€æŸ¥å®é™… reward çš„åˆ†å¸ƒ

## ğŸ”§ ä¿®å¤å»ºè®®

### ä¿®å¤1ï¼šç§»é™¤æˆ–åè½¬è‡ªé€‚åº” sigma

**æ–¹æ¡ˆAï¼šä½¿ç”¨å›ºå®š sigmaï¼ˆæ¨èï¼‰**
```python
# ç®€å•ã€ç¨³å®šã€æ˜“äºè°ƒè¯•
rew_per_finger = torch.exp(-diff_per_finger / (base_sigma + 1e-6))
```

**æ–¹æ¡ˆBï¼šåè½¬è‡ªé€‚åº” sigmaï¼ˆå¦‚æœç¡®å®éœ€è¦è‡ªé€‚åº”ï¼‰**
```python
# è¯¯å·®å¤§æ—¶ï¼Œsigma å˜å°ï¼Œå¥–åŠ±è¡°å‡æ›´å¿«
sigma_scale = 1.0 - 0.7 * (diff_per_finger / (diff_per_finger + adaptive_threshold))
adaptive_sigma = torch.clamp(base_sigma * sigma_scale, min=base_sigma * 0.3, max=base_sigma * 1.0)
```

### ä¿®å¤2ï¼šè°ƒæ•´ reward å…¬å¼

**ç§»é™¤ç³»æ•° 2**ï¼š
```python
# æ›´å¹³æ»‘çš„å¥–åŠ±è¡°å‡
rew_per_finger = torch.exp(-diff_per_finger / (sigma + 1e-6))
```

**æˆ–è€…ä½¿ç”¨å¹³æ–¹è¯¯å·®**ï¼š
```python
# æ›´æ ‡å‡†çš„ reward å½¢å¼
rew_per_finger = torch.exp(-diff_per_finger / (sigma + 1e-6))
```

### ä¿®å¤3ï¼šæ”¹è¿›å¤šæ‰‹æŒ‡èšåˆæ–¹å¼

**æ–¹æ¡ˆAï¼šä½¿ç”¨æœ€å°å¥–åŠ±ï¼ˆæ›´ä¸¥æ ¼ï¼‰**
```python
rew = torch.min(rew_per_finger, dim=1)[0]  # æ‰€æœ‰æ‰‹æŒ‡éƒ½è¦å¥½
```

**æ–¹æ¡ˆBï¼šä½¿ç”¨åŠ æƒå¹³å‡**
```python
# ç»™æ¯ä¸ªæ‰‹æŒ‡ä¸åŒçš„æƒé‡
finger_weights = torch.ones(num_fingers, device=self.device) / num_fingers
rew = torch.sum(rew_per_finger * finger_weights.unsqueeze(0), dim=1)
```

**æ–¹æ¡ˆCï¼šä½¿ç”¨å‡ ä½•å¹³å‡ï¼ˆæ›´å¹³è¡¡ï¼‰**
```python
rew = torch.prod(rew_per_finger, dim=1) ** (1.0 / num_fingers)
```

### ä¿®å¤4ï¼šæ·»åŠ  reward å½’ä¸€åŒ–

**åœ¨ compute_reward ä¸­æ·»åŠ **ï¼š
```python
# å½’ä¸€åŒ– reward åˆ°åˆç†èŒƒå›´
self.rew_buf = torch.clamp(self.rew_buf, min=-10.0, max=10.0)
```

## ğŸ“Š è¯Šæ–­å»ºè®®

### 1. æ£€æŸ¥ reward åˆ†å¸ƒ

æ·»åŠ ä»£ç è®°å½• reward çš„ç»Ÿè®¡ä¿¡æ¯ï¼š
```python
def compute_reward(self):
    # ... existing code ...
    
    # æ·»åŠ è¯Šæ–­ä¿¡æ¯
    if self.global_steps % 100 == 0:
        print(f"Reward stats: mean={self.rew_buf.mean():.4f}, std={self.rew_buf.std():.4f}, "
              f"min={self.rew_buf.min():.4f}, max={self.rew_buf.max():.4f}")
```

### 2. æ£€æŸ¥ value function é¢„æµ‹è¯¯å·®

åœ¨è®­ç»ƒæ—¥å¿—ä¸­å…³æ³¨ï¼š
- Value loss çš„å¤§å°å’Œå˜åŒ–è¶‹åŠ¿
- Value prediction å’Œå®é™… return çš„å·®å¼‚
- Reward çš„æ–¹å·®

### 3. å¯è§†åŒ– reward å‡½æ•°

ç»˜åˆ¶ reward éšè¯¯å·®å˜åŒ–çš„æ›²çº¿ï¼Œæ£€æŸ¥æ˜¯å¦å¹³æ»‘

## ğŸ¯ æ¨èçš„ä¿®å¤æ–¹æ¡ˆï¼ˆä¼˜å…ˆçº§æ’åºï¼‰

1. **ç«‹å³ä¿®å¤**ï¼šç§»é™¤è‡ªé€‚åº” sigmaï¼Œä½¿ç”¨å›ºå®š sigma
2. **ç«‹å³ä¿®å¤**ï¼šç§»é™¤ reward å…¬å¼ä¸­çš„ç³»æ•° 2
3. **è€ƒè™‘ä¿®å¤**ï¼šæ”¹ç”¨æœ€å°å¥–åŠ±è€Œä¸æ˜¯å¹³å‡
4. **é•¿æœŸä¼˜åŒ–**ï¼šæ·»åŠ  reward å½’ä¸€åŒ–å’Œè¯Šæ–­å·¥å…·



