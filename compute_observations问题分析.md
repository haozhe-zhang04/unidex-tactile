# compute_observations å‡½æ•°é—®é¢˜åˆ†æ

## ğŸ”´ å‘ç°çš„ä¸¥é‡é—®é¢˜

### é—®é¢˜1ï¼šé™¤é›¶é£é™© - gripper_force_kpsï¼ˆæœ€ä¸¥é‡ï¼‰

**ä½ç½®**ï¼šç¬¬558è¡Œ
```python
curr_ee_goal_cart_world_offset = forces_offset_global / self.gripper_force_kps + ...
```

**é—®é¢˜**ï¼š
- `gripper_force_kps` å½¢çŠ¶æ˜¯ `(num_envs, num_fingers, 3)`
- å¦‚æœæŸä¸ªç¯å¢ƒçš„æŸä¸ªæ‰‹æŒ‡çš„æŸä¸ªç»´åº¦çš„ kp æ¥è¿‘ 0ï¼Œä¼šå¯¼è‡´é™¤é›¶æˆ–éå¸¸å¤§çš„å€¼
- è¿™ä¼šå¯¼è‡´ observation ä¸­å‡ºç°å¼‚å¸¸å¤§çš„å€¼ï¼Œvalue function æ— æ³•é¢„æµ‹

**ä¿®å¤**ï¼š
```python
# æ·»åŠ ä¿æŠ¤ï¼Œé¿å…é™¤é›¶
gripper_force_kps_safe = torch.clamp(self.gripper_force_kps, min=1e-3)
curr_ee_goal_cart_world_offset = forces_offset_global / gripper_force_kps_safe + ...
```

### é—®é¢˜2ï¼šå½’ä¸€åŒ–é™¤é›¶é£é™©

**ä½ç½®**ï¼š`_normalize_pos` å‡½æ•°ï¼ˆ1179-1181è¡Œï¼‰
```python
normalized_pos[:,i,0:1] = 2 * (pos[:,i,0:1] - min[i]) / (max[i] - min[i]) - 1
```

**é—®é¢˜**ï¼š
- å¦‚æœ `max[i] - min[i] = 0`ï¼Œä¼šå¯¼è‡´é™¤é›¶
- å³ä½¿ä¸é™¤é›¶ï¼Œå¦‚æœå·®å€¼å¾ˆå°ï¼Œå½’ä¸€åŒ–åçš„å€¼ä¼šéå¸¸å¤§

**ä¿®å¤**ï¼š
```python
def _normalize_pos(self, pos):
    assert pos.shape ==(self.num_envs, len(self.finger_tips_idx), 3)
    normalized_pos = torch.zeros_like(pos)
    for i in range(len(self.finger_tips_idx)):
        x_range = self.cfg.normalization.obs_scales.finger_tip_pos_x_max[i] - self.cfg.normalization.obs_scales.finger_tip_pos_x_min[i]
        y_range = self.cfg.normalization.obs_scales.finger_tip_pos_y_max[i] - self.cfg.normalization.obs_scales.finger_tip_pos_y_min[i]
        z_range = self.cfg.normalization.obs_scales.finger_tip_pos_z_max[i] - self.cfg.normalization.obs_scales.finger_tip_pos_z_min[i]
        
        # æ·»åŠ ä¿æŠ¤ï¼Œé¿å…é™¤é›¶
        x_range = torch.clamp(torch.tensor(x_range), min=1e-6)
        y_range = torch.clamp(torch.tensor(y_range), min=1e-6)
        z_range = torch.clamp(torch.tensor(z_range), min=1e-6)
        
        normalized_pos[:,i,0:1] = 2 * (pos[:,i,0:1] - self.cfg.normalization.obs_scales.finger_tip_pos_x_min[i]) / x_range - 1
        normalized_pos[:,i,1:2] = 2 * (pos[:,i,1:2] - self.cfg.normalization.obs_scales.finger_tip_pos_y_min[i]) / y_range - 1
        normalized_pos[:,i,2:3] = 2 * (pos[:,i,2:3] - self.cfg.normalization.obs_scales.finger_tip_pos_z_min[i]) / z_range - 1
        
        # è£å‰ªåˆ°åˆç†èŒƒå›´
        normalized_pos[:,i,:] = torch.clamp(normalized_pos[:,i,:], min=-10.0, max=10.0)
    
    return normalized_pos
```

### é—®é¢˜3ï¼š6Dæ—‹è½¬è¡¨ç¤ºå¯èƒ½ä¸ç¨³å®š

**ä½ç½®**ï¼šç¬¬546è¡Œå’Œ564è¡Œ
```python
finger_tip_orn_6d_base = quat_to_mat6d(finger_tip_orn_quat_base)
finger_tip_orn_6d_error = finger_tip_orn_6d_base - curr_finger_tip_goal_orn_6d_base
```

**é—®é¢˜**ï¼š
- å¦‚æœå››å…ƒæ•°æ²¡æœ‰æ­£ç¡®å½’ä¸€åŒ–ï¼Œ`quat_to_mat6d` å¯èƒ½äº§ç”Ÿå¼‚å¸¸å€¼
- 6D è¡¨ç¤ºçš„èŒƒå›´æ˜¯ [-1, 1]ï¼Œä½†å¦‚æœæ—‹è½¬çŸ©é˜µä¸åˆæ³•ï¼Œå¯èƒ½è¶…å‡ºèŒƒå›´

**ä¿®å¤**ï¼š
```python
# ç¡®ä¿å››å…ƒæ•°å½’ä¸€åŒ–
finger_tip_orn_quat_base = finger_tip_orn_quat_base / (torch.norm(finger_tip_orn_quat_base, dim=-1, keepdim=True) + 1e-8)
finger_tip_orn_6d_base = quat_to_mat6d(finger_tip_orn_quat_base)

# è£å‰ª6Dè¡¨ç¤ºåˆ°åˆç†èŒƒå›´
finger_tip_orn_6d_base = torch.clamp(finger_tip_orn_6d_base, min=-2.0, max=2.0)
```

### é—®é¢˜4ï¼šObservation clip å¯èƒ½å¤ªä¸¥æ ¼

**ä½ç½®**ï¼šç¬¬348-352è¡Œ
```python
clip_obs = self.cfg.normalization.clip_observations  # é»˜è®¤æ˜¯ 1.0
self.obs_buf = torch.clip(self.obs_buf, -clip_obs, clip_obs)
```

**é—®é¢˜**ï¼š
- `clip_observations = 1.0` å¯èƒ½å¤ªå°
- å¦‚æœæŸäº› observation å€¼è¶…è¿‡ 1.0ï¼Œä¼šè¢«è£å‰ªï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±
- è¿™ä¼šè®© value function å­¦ä¹ åˆ°ä¸è¿ç»­çš„å‡½æ•°

**å»ºè®®**ï¼š
- æ£€æŸ¥å®é™…çš„ observation èŒƒå›´
- å¦‚æœç»å¸¸è¢« clipï¼Œè€ƒè™‘å¢å¤§ `clip_observations` æˆ–è°ƒæ•´ normalization

### é—®é¢˜5ï¼šå››å…ƒæ•°å½’ä¸€åŒ–å¯èƒ½ä¸å……åˆ†

**ä½ç½®**ï¼šç¬¬545è¡Œ
```python
finger_tip_orn_quat_base = quat_mul(quat_conjugate(base_quat_reshaped), finger_tip_orn_quat_world.reshape(-1,4)).reshape(self.num_envs,num_fingers,4)
```

**é—®é¢˜**ï¼š
- `quat_mul` åæ²¡æœ‰æ˜¾å¼å½’ä¸€åŒ–
- è™½ç„¶ç†è®ºä¸Šåº”è¯¥ä¿æŒå½’ä¸€åŒ–ï¼Œä½†æ•°å€¼è¯¯å·®å¯èƒ½å¯¼è‡´ä¸å½’ä¸€åŒ–

**ä¿®å¤**ï¼š
```python
finger_tip_orn_quat_base = quat_mul(quat_conjugate(base_quat_reshaped), finger_tip_orn_quat_world.reshape(-1,4)).reshape(self.num_envs,num_fingers,4)
# æ˜¾å¼å½’ä¸€åŒ–
finger_tip_orn_quat_base = finger_tip_orn_quat_base / (torch.norm(finger_tip_orn_quat_base, dim=-1, keepdim=True) + 1e-8)
```

### é—®é¢˜6ï¼šNaN/Inf æ£€æŸ¥ç¼ºå¤±

**é—®é¢˜**ï¼š
- æ²¡æœ‰æ£€æŸ¥ observation ä¸­æ˜¯å¦æœ‰ NaN æˆ– Inf
- è¿™äº›å€¼ä¼šå¯¼è‡´ value function è®­ç»ƒå´©æºƒ

**ä¿®å¤**ï¼š
```python
# åœ¨ compute_observations æœ€åæ·»åŠ æ£€æŸ¥
if torch.any(torch.isnan(self.obs_buf)) or torch.any(torch.isinf(self.obs_buf)):
    print(f"Warning: NaN or Inf in obs_buf at step {self.global_steps}")
    self.obs_buf = torch.nan_to_num(self.obs_buf, nan=0.0, posinf=1.0, neginf=-1.0)
```

## ğŸ”§ æ¨èçš„ä¿®å¤é¡ºåº

1. **ç«‹å³ä¿®å¤**ï¼šæ·»åŠ  gripper_force_kps çš„é™¤é›¶ä¿æŠ¤
2. **ç«‹å³ä¿®å¤**ï¼šæ·»åŠ å½’ä¸€åŒ–çš„é™¤é›¶ä¿æŠ¤
3. **ç«‹å³ä¿®å¤**ï¼šæ·»åŠ  NaN/Inf æ£€æŸ¥
4. **è€ƒè™‘ä¿®å¤**ï¼šæ˜¾å¼å½’ä¸€åŒ–å››å…ƒæ•°
5. **é•¿æœŸä¼˜åŒ–**ï¼šæ£€æŸ¥å¹¶è°ƒæ•´ clip_observations



